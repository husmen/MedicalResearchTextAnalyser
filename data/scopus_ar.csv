"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Page count","Cited by","DOI","Link","Abstract","Author Keywords","Index Keywords","Document Type","Publication Stage","Open Access","Source","EID"
"Hou X.; Sang G.; Liu Z.; Li X.; Zhang Y.","Hou, Xiaodi (58629298500); Sang, Guoming (26538170900); Liu, Zhi (57205074661); Li, Xiaobo (58091342200); Zhang, Yijia (37762522900)","58629298500; 26538170900; 57205074661; 58091342200; 37762522900","Radiology Report Generation via Visual Recalibration and Context Gating-Aware","2023","Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","14248 LNBI","","","107","119","12","0","10.1007/978-981-99-7074-2_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174313430&doi=10.1007%2f978-981-99-7074-2_9&partnerID=40&md5=8800e735c17448f2abb5c76fce77275b","The task of radiology report generation aims to analyze medical images, extract key information, and then assist medical personnel in generating detailed and accurate reports. Therefore, automatic radiology report generation plays an important role in medical diagnosis and healthcare. However, radiology medical data face the problems of visual and text data bias: medical images are similar to each other, and the normal feature distribution is larger than the abnormal feature distribution; second, the accurate location of the lesion and the generation of accurate and coherent long text reports are important challenges. In this paper, we propose Visual Recalibration and Context Gating-aware model (VRCG) to alleviate visual and textual data bias for enhancing report generation. We employ a medical visual recalibration module to enhance the key lesion feature extraction. We use the context gating-aware module to combine lesion location and report context information to solve the problem of long-distance dependence in diagnostic reports. Meanwhile, the context gating-aware module can identify text fragments related to lesion descriptions, improve the model’s perception of lesion text information, and then generate coherent, consistent medical reporting. Extensive experiments demonstrate that our proposed model outperforms existing baseline models on a publicly available IU X-Ray dataset. The source code is available at: https://github.com/Eleanorhxd/VRCG. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Context Gating-aware; Medical Visual Recalibration; Report Generation","Data mining; Diagnosis; Medical imaging; Medical problems; Context gating-aware; Feature distribution; Medical data; Medical personnel; Medical visual recalibration; Radiology reports; Recalibrations; Report generation; Text data; Visual data; Radiology","Conference paper","Final","","Scopus","2-s2.0-85174313430"
"Tanida T.; Müller P.; Kaissis G.; Rueckert D.","Tanida, Tim (58258373300); Müller, Philip (57222251473); Kaissis, Georgios (56600872600); Rueckert, Daniel (7004895812)","58258373300; 57222251473; 56600872600; 7004895812","Interactive and Explainable Region-guided Radiology Report Generation","2023","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2023-June","","","7433","7442","9","0","10.1109/CVPR52729.2023.00718","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166345730&doi=10.1109%2fCVPR52729.2023.00718&partnerID=40&md5=64b5eb4fe78a3e9229d1f65c029106f6","The automatic generation of radiology reports has the potential to assist radiologists in the time-consuming task of report writing. Existing methods generate the full report from image-level features, failing to explicitly focus on anatomical regions in the image. We propose a simple yet effective region-guided report generation model that detects anatomical regions and then describes individual, salient regions to form the final report. While previous methods generate reports without the possibility of human intervention and with limited explainability, our method opens up novel clinical use cases through additional interactive capabilities and introduces a high degree of transparency and explainability. Comprehensive experiments demonstrate our method's effectiveness in report generation, outperforming previous state-of-the-art models, and highlight its interactive capabilities. The code and checkpoints are available at https://github.com/ttanida/rgrg. © 2023 IEEE.","cell microscopy; Medical and biological vision","","Conference paper","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85166345730"
"Zhao S.; Li Q.; Yang Y.; Wen J.; Luo W.","Zhao, Shuai (57214168521); Li, Qing (58463798400); Yang, Yuer (57221970551); Wen, Jinming (55800595000); Luo, Weiqi (57194349411)","57214168521; 58463798400; 57221970551; 55800595000; 57194349411","From Softmax to Nucleusmax: A Novel Sparse Language Model for Chinese Radiology Report Summarization","2023","ACM Transactions on Asian and Low-Resource Language Information Processing","22","6","3596219","","","","3","10.1145/3596219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164235257&doi=10.1145%2f3596219&partnerID=40&md5=1b2232b3a03135bf89ec7674adfd0468","The Chinese radiology report summarization is a crucial component in smart healthcare that employs language models to summarize key findings in radiology reports and communicate these findings to physicians. However, most language models for radiology report summarization utilize a softmax transformation in their output layer, leading to dense alignments and strictly positive output probabilities. This density is inefficient, reducing model interpretability and giving probability mass to many unrealistic outputs. To tackle this issue, we propose a novel approach named nucleusmax. Nucleusmax is able to mitigate dense outputs and improve model interpretability by truncating the unreliable tail of the probability distribution. In addition, we incorporate nucleusmax with a copy mechanism, a useful technique to avoid professional errors in the generated diagnostic opinions. To further promote the research of radiology report summarization, we also have created a Chinese radiology report summarization dataset, which is freely available. Experimental results showed via both automatic and human evaluation that the proposed approach substantially improves the sparsity and overall quality of outputs over competitive softmax models, producing radiology summaries that approach the quality of those authored by physicians. In general, our work demonstrates the feasibility and prospect of the language model to the domain of radiology and smart healthcare.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.","abstractive summarization; Additional Key Words and PhrasesChinese radiology report summarization; language model; softmax","Computational linguistics; Health care; Probability distributions; Abstractive summarization; Additional key word and phraseschinese radiology report summarization; Dense output; Interpretability; Key words; Language model; Output layer; Radiology reports; Softmax; Sparse languages; Radiology","Article","Final","","Scopus","2-s2.0-85164235257"
"Hou X.; Liu Z.; Li X.; Li X.; Sang S.; Zhang Y.","Hou, Xiaodi (58629298500); Liu, Zhi (57205074661); Li, Xiaobo (58091342200); Li, Xingwang (57375223000); Sang, Shengtian (56828922200); Zhang, Yijia (37762522900)","58629298500; 57205074661; 58091342200; 57375223000; 56828922200; 37762522900","MKCL: Medical Knowledge with Contrastive Learning model for radiology report generation","2023","Journal of Biomedical Informatics","146","","104496","","","","0","10.1016/j.jbi.2023.104496","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172987099&doi=10.1016%2fj.jbi.2023.104496&partnerID=40&md5=d42e5d977bc750713688e3768193d45c","Automatic radiology report generation has the potential to alert inexperienced radiologists to misdiagnoses or missed diagnoses and improve healthcare delivery efficiency by reducing the documentation workload of radiologists. Motivated by the continuous development of automatic image captioning, more and more deep learning methods have been proposed for automatic radiology report generation. However, the visual and textual data bias problem still face many challenges in the medical domain. Additionally, do not integrate medical knowledge, ignoring the mutual influences between medical findings, and abundant unlabeled medical images influence the accuracy of generating report. In this paper, we propose a Medical Knowledge with Contrastive Learning model (MKCL) to enhance radiology report generation. The proposed model MKCL uses IU Medical Knowledge Graph (IU-MKG) to mine the relationship among medical findings and improve the accuracy of identifying positive diseases findings from radiologic medical images. In particular, we design Knowledge Enhanced Attention (KEA), which integrates the IU-MKG and the extracted chest radiological visual features to alleviate textual data bias. Meanwhile, this paper leverages supervised contrastive learning to relieve radiographic medical images which have not been labeled, and identify abnormalities from images. Experimental results on the public dataset IU X-ray show that our proposed model MKCL outperforms other state-of-the-art report generation methods. Ablation studies also demonstrate that IU medical knowledge graph module and supervised contrastive learning module enhance the ability of the model to detect the abnormal parts and accurately describe the abnormal findings. The source code is available at: https://github.com/Eleanorhxd/MKCL. © 2023 Elsevier Inc.","Medical knowledge graph; Radiology report generation; Supervised contrastive learning","Diagnosis; Image enhancement; Knowledge graph; Learning systems; Medical imaging; Radiology; Healthcare delivery; Knowledge graphs; Learning models; Medical knowledge; Medical knowledge graph; Radiology report generation; Radiology reports; Report generation; Supervised contrastive learning; Textual data; accuracy; Article; automation; computer model; health care delivery; human; image analysis; knowledge; learning; Medical Knowledge with Contrastive Learning model; radiodiagnosis; radiology; reporting and data system; Deep learning","Article","Final","","Scopus","2-s2.0-85172987099"
"Kale K.; Bhattacharyya P.; Shetty A.; Gune M.; Shrivastava K.; Lawyer R.; Biswas S.","Kale, Kaveri (57766457200); Bhattacharyya, Pushpak (7101803108); Shetty, Aditya (57221834802); Gune, Milind (6507129306); Shrivastava, Kush (57766020900); Lawyer, Rustom (57765804700); Biswas, Spriha (57766021000)","57766457200; 7101803108; 57221834802; 6507129306; 57766020900; 57765804700; 57766021000","Knowledge is Power"": Constructing Knowledge Graph of Abdominal Organs and Using Them for Automatic Radiology Report Generation","2023","Proceedings of the Annual Meeting of the Association for Computational Linguistics","5","","","11","24","13","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173236371&partnerID=40&md5=bacabd119927dcb2d93faf6f5d8feedd","In conventional radiology practice, the radiologist dictates the diagnosis to the transcription-ist, who then prepares a preliminary formatted report referring to the notes, after which the radiologist reviews the report, corrects the errors, and signs off. This workfow is prone to delay and error. In this paper, we report our work on automatic radiology report generation from radiologists' dictation, which is in collaboration with a startup about to become Unicorn. A major contribution of our work is the set of knowledge graphs (KGs) of ten abdominal organs- Liver, Kidney, Gallbladder, Uterus, Urinary bladder, Ovary, Pancreas, Prostate, Biliary Tree, and Bowel. Our method for constructing these KGs relies on extracting entity1-relation-entity2 triplets from a large collection (about 10,000) of free-text radiology reports. The quality and coverage of the KGs are veri-fed by two experienced radiologists (practicing for the last 30 years and 8 years, respectively). The dictation of the radiologist is automatically converted to what is called a pathological description which is the clinical description of the fndings of the radiologist during ultrasonog-raphy (USG). Our knowledge-enhanced deep learning model improves the reported BLEU-3, ROUGE-L, METEOR, and CIDEr scores of the pathological description generation by 2%, 4%, 2% and 2% respectively. To the best of our knowledge, this is the frst attempt at representing the abdominal organs in the form of knowledge graphs and utilising these graphs for the automatic generation of USG reports. A Minimum Viable Product (MVP) has been made available to the beta users, i.e., radiologists of reputed hospitals, for testing and evaluation. Our solution guarantees report generation within 30 seconds of running a scan. © ACL 2023.All rights reserved.","","Knowledge graph; Radiology; Trees (mathematics); Abdominal organs; Biliary tree; Free texts; Knowledge graphs; Learning models; Power; Radiology reports; Report generation; Sign-off; Urinary bladder; Deep learning","Conference paper","Final","","Scopus","2-s2.0-85173236371"
"Kourav M.; Agarwal S.; Arya K.V.; Petrlik I.; Rodriguez C.","Kourav, Mohit (58575718800); Agarwal, Saurabh (58276999700); Arya, K.V. (54891034900); Petrlik, Ivan (58360237700); Rodriguez, Ciro (57211553292)","58575718800; 58276999700; 54891034900; 58360237700; 57211553292","Automatic Chest Radiology Report Generation Using Reinforcement Learning","2023","Lecture Notes in Networks and Systems","685 LNNS","","","239","247","8","0","10.1007/978-981-99-1912-3_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171150240&doi=10.1007%2f978-981-99-1912-3_22&partnerID=40&md5=1010b55265c82356e7faebfb1c2347e7","Radiology report generation is the manual task assigned to radiologists which analyze the given chest image (x-ray) of patient and generates the report based on that analysis. Many of the research is undergoing in this field to make this process automatic. However, there are a large number of difficulties which have been faced by the researchers while developing the efficient automatic report generation model. The main difficulties include the generation of clinically inaccurate reports when analyzed with the reports of the expert radiologist. The introduction of deep learning based approaches achieved tremendous advancement in these areas and can produce chest X-ray reports in a more accurate manner. BY further optimizing the deep learning based approaches, we proposed a reinforcement learning-based model that generates automated X-ray reports based on input chest X-ray images. We also developed a lookahead inference mechanism to overcome the limitation of beam search, which selects words based on policy and value networks at each particular step of time. The proposed model achieved 0.454, 0.546, 0.466, 0.410, and 0.424 for the Bleu-1, Bleu-2, Bleu-3, Bleu-4, and Rouge evaluation metrics, respectively, surpassing all available state-of-the-art algorithms. © 2023, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.","Chest radiology report generation; Inference mechanism; Reinforcement learning","Deep learning; Radiology; Beam search; Chest image; Chest radiology report generation; Chest X-ray image; Inference mechanism; Learning Based Models; Learning-based approach; Radiology reports; Reinforcement learnings; Report generation; Reinforcement learning","Conference paper","Final","","Scopus","2-s2.0-85171150240"
"Yu F.; Endo M.; Krishnan R.; Pan I.; Tsai A.; Reis E.P.; Fonseca E.K.U.N.; Lee H.M.H.; Abad Z.S.H.; Ng A.Y.; Langlotz C.P.; Venugopal V.K.; Rajpurkar P.","Yu, Feiyang (57900108400); Endo, Mark (57222288524); Krishnan, Rayan (57219759103); Pan, Ian (57205526888); Tsai, Andy (35197308600); Reis, Eduardo Pontes (57218595708); Fonseca, Eduardo Kaiser Ururahy Nunes (56463625700); Lee, Henrique Min Ho (57841418000); Abad, Zahra Shakeri Hossein (36986914600); Ng, Andrew Y. (35410071600); Langlotz, Curtis P. (20134955200); Venugopal, Vasantha Kumar (57209063288); Rajpurkar, Pranav (57056352800)","57900108400; 57222288524; 57219759103; 57205526888; 35197308600; 57218595708; 56463625700; 57841418000; 36986914600; 35410071600; 20134955200; 57209063288; 57056352800","Evaluating progress in automatic chest X-ray radiology report generation","2023","Patterns","4","9","100802","","","","1","10.1016/j.patter.2023.100802","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169793819&doi=10.1016%2fj.patter.2023.100802&partnerID=40&md5=2c0a3dd1bdd9d37ced5ccc7b084b873e","Artificial intelligence (AI) models for automatic generation of narrative radiology reports from images have the potential to enhance efficiency and reduce the workload of radiologists. However, evaluating the correctness of these reports requires metrics that can capture clinically pertinent differences. In this study, we investigate the alignment between automated metrics and radiologists' scoring of errors in report generation. We address the limitations of existing metrics by proposing new metrics, RadGraph F1 and RadCliQ, which demonstrate stronger correlation with radiologists' evaluations. In addition, we analyze the failure modes of the metrics to understand their limitations and provide guidance for metric selection and interpretation. This study establishes RadGraph F1 and RadCliQ as meaningful metrics for guiding future research in radiology report generation. © 2023","alignment with radiologists; automatic metrics; chest X-ray radiology report generation; DSML 2: Proof-of-concept: Data science output has been formulated, implemented, and tested for one domain/problem","Radiology; Alignment with radiologist; Automatic metrics; Chest X-ray radiology report generation; Domain problems; DSML 2: proof-of-concept: data science output have been formulated, implemented, and tested for one domain/problem; Intelligence models; Proof of concept; Radiology reports; Report generation; X-ray radiology; Image enhancement","Article","Final","All Open Access; Gold Open Access; Green Open Access","Scopus","2-s2.0-85169793819"
"Yang S.; Wu X.; Ge S.; Zheng Z.; Zhou S.K.; Xiao L.","Yang, Shuxin (57224939140); Wu, Xian (57209291701); Ge, Shen (57211755485); Zheng, Zhuozhao (15137845800); Zhou, S. Kevin (7404165802); Xiao, Li (57208199724)","57224939140; 57209291701; 57211755485; 15137845800; 7404165802; 57208199724","Radiology report generation with a learned knowledge base and multi-modal alignment","2023","Medical Image Analysis","86","","102798","","","","1","10.1016/j.media.2023.102798","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151008526&doi=10.1016%2fj.media.2023.102798&partnerID=40&md5=d833d037e029fa7162e0a3e90afd6d19","In clinics, a radiology report is crucial for guiding a patient's treatment. However, writing radiology reports is a heavy burden for radiologists. To this end, we present an automatic, multi-modal approach for report generation from a chest x-ray. Our approach, motivated by the observation that the descriptions in radiology reports are highly correlated with specific information of the x-ray images, features two distinct modules: (i) Learned knowledge base: To absorb the knowledge embedded in the radiology reports, we build a knowledge base that can automatically distill and restore medical knowledge from textual embedding without manual labor; (ii) Multi-modal alignment: to promote the semantic alignment among reports, disease labels, and images, we explicitly utilize textual embedding to guide the learning of the visual feature space. We evaluate the performance of the proposed model using metrics from both natural language generation and clinic efficacy on the public IU-Xray and MIMIC-CXR datasets. Our ablation study shows that each module contributes to improving the quality of generated reports. Furthermore, the assistance of both modules, our approach outperforms state-of-the-art methods over almost all the metrics. Code is available at https://github.com/LX-doctorAI1/M2KT. © 2023 The Author(s)","Knowledge base; Multi-modal alignment; Radiology report generation","Benchmarking; Humans; Knowledge Bases; Learning; Radiography; Radiology; Alignment; Knowledge based systems; Medical imaging; Natural language processing systems; Patient treatment; Radiology; Semantics; Chest x-rays; Embeddings; Highly-correlated; Knowledge base; Multi-modal; Multi-modal alignment; Multi-modal approach; Radiology report generation; Radiology reports; Report generation; Article; embedding; human; human experiment; knowledge base; language; learning; manual labor; radiology; thorax radiography; X ray; benchmarking; knowledge base; radiography; Embeddings","Article","Final","All Open Access; Green Open Access; Hybrid Gold Open Access","Scopus","2-s2.0-85151008526"
"Moezzi S.A.R.; Ghaedi A.; Rahmanian M.; Mousavi S.Z.; Sami A.","Moezzi, Seyed Ali Reza (57191893121); Ghaedi, Abdolrahman (57866519500); Rahmanian, Mojdeh (57225364026); Mousavi, Seyedeh Zahra (57219619932); Sami, Ashkan (7004124604)","57191893121; 57866519500; 57225364026; 57219619932; 7004124604","Application of Deep Learning in Generating Structured Radiology Reports: A Transformer-Based Technique","2023","Journal of Digital Imaging","36","1","","80","90","10","0","10.1007/s10278-022-00692-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136943469&doi=10.1007%2fs10278-022-00692-x&partnerID=40&md5=043f1421e3ef76720aa99ffea80852c3","Since radiology reports needed for clinical practice and research are written and stored in free-text narrations, extraction of relative information for further analysis is difficult. In these circumstances, natural language processing (NLP) techniques can facilitate automatic information extraction and transformation of free-text formats to structured data. In recent years, deep learning (DL)-based models have been adapted for NLP experiments with promising results. Despite the significant potential of DL models based on artificial neural networks (ANN) and convolutional neural networks (CNN), the models face some limitations to implement in clinical practice. Transformers, another new DL architecture, have been increasingly applied to improve the process. Therefore, in this study, we propose a transformer-based fine-grained named entity recognition (NER) architecture for clinical information extraction. We collected 88 abdominopelvic sonography reports in free-text formats and annotated them based on our developed information schema. The text-to-text transfer transformer model (T5) and Scifive, a pre-trained domain-specific adaptation of the T5 model, were applied for fine-tuning to extract entities and relations and transform the input into a structured format. Our transformer-based model in this study outperformed previously applied approaches such as ANN and CNN models based on ROUGE-1, ROUGE-2, ROUGE-L, and BLEU scores of 0.816, 0.668, 0.528, and 0.743, respectively, while providing an interpretable structured report. © 2022, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine.","Deep learning; Named entity recognition; Natural language processing; Relation extraction; Structured reporting; Transformers","Deep Learning; Humans; Information Storage and Retrieval; Natural Language Processing; Neural Networks, Computer; Radiography; Radiology; Clinical research; Convolutional neural networks; Information retrieval; Metadata; Natural language processing systems; Network architecture; Neural network models; Radiation; Radiology; Ultrasonic imaging; Deep learning; Free texts; Language processing; Named entity recognition; Natural language processing; Natural languages; Radiology reports; Relation extraction; Structured reporting; Transformer; article; artificial neural network; clinical practice; convolutional neural network; deep learning; echography; extraction; human; natural language processing; radiology; information retrieval; natural language processing; radiography; Deep learning","Article","Final","All Open Access; Green Open Access","Scopus","2-s2.0-85136943469"
"Chambon P.J.; Wu C.; Steinkamp J.M.; Adleberg J.; Cook T.S.; Langlotz C.P.","Chambon, Pierre J. (57226150182); Wu, Christopher (58070257000); Steinkamp, Jackson M. (57200859444); Adleberg, Jason (57194592917); Cook, Tessa S. (36144992200); Langlotz, Curtis P. (20134955200)","57226150182; 58070257000; 57200859444; 57194592917; 36144992200; 20134955200","Automated deidentification of radiology reports combining transformer and ""hide in plain sight"" rule-based methods","2023","Journal of the American Medical Informatics Association : JAMIA","30","2","","318","328","10","1","10.1093/jamia/ocac219","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146484482&doi=10.1093%2fjamia%2focac219&partnerID=40&md5=dd9bfe2822ab74f725c10f43590cdb80","OBJECTIVE: To develop an automated deidentification pipeline for radiology reports that detect protected health information (PHI) entities and replaces them with realistic surrogates ""hiding in plain sight."" MATERIALS AND METHODS: In this retrospective study, 999 chest X-ray and CT reports collected between November 2019 and November 2020 were annotated for PHI at the token level and combined with 3001 X-rays and 2193 medical notes previously labeled, forming a large multi-institutional and cross-domain dataset of 6193 documents. Two radiology test sets, from a known and a new institution, as well as i2b2 2006 and 2014 test sets, served as an evaluation set to estimate model performance and to compare it with previously released deidentification tools. Several PHI detection models were developed based on different training datasets, fine-tuning approaches and data augmentation techniques, and a synthetic PHI generation algorithm. These models were compared using metrics such as precision, recall and F1 score, as well as paired samples Wilcoxon tests. RESULTS: Our best PHI detection model achieves 97.9 F1 score on radiology reports from a known institution, 99.6 from a new institution, 99.5 on i2b2 2006, and 98.9 on i2b2 2014. On reports from a known institution, it achieves 99.1 recall of detecting the core of each PHI span. DISCUSSION: Our model outperforms all deidentifiers it was compared to on all test sets as well as human labelers on i2b2 2014 data. It enables accurate and automatic deidentification of radiology reports. CONCLUSIONS: A transformer-based deidentification pipeline can achieve state-of-the-art performance for deidentifying radiology reports and other medical documents. © The Author(s) 2022. Published by Oxford University Press on behalf of the American Medical Informatics Association. All rights reserved. For permissions, please email: journals.permissions@oup.com.","deidentification; machine learning; NLP; radiology; transformer","Algorithms; Data Anonymization; Health Facilities; Humans; Natural Language Processing; Radiology; Retrospective Studies; algorithm; anonymization; health care facility; human; natural language processing; radiology; retrospective study","Article","Final","","Scopus","2-s2.0-85146484482"
