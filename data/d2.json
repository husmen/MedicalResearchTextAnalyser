{"results": [{"authors": "Liao Y.; Liu H.; Spasi\u0107 I.", "author full names": "Liao, Yuxiang (58285135800); Liu, Hantao (55581726800); Spasi\u0107, Irena (8645090100)", "author(s) id": "58285135800; 55581726800; 8645090100", "title": "Deep learning approaches to automatic radiology report generation: A systematic review", "year": 2023, "source title": "Informatics in Medicine Unlocked", "volume": 39, "issue": NaN, "art. no.": 101273.0, "page start": NaN, "page end": NaN, "page count": NaN, "cited by": 0, "doi": "10.1016/j.imu.2023.101273", "link": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160016936&doi=10.1016%2fj.imu.2023.101273&partnerID=40&md5=7d98c605bd9c9ffd817c725df5bd46a2", "abstract": "Background: A radiology report communicates the imaging findings to the referring clinicians. The rising number of referrals has created a bottleneck in healthcare. Writing a report takes disproportionally more time than the imaging itself. Therefore, Automatic Radiology Report Generation (ARRG) has a great potential to unclog this bottleneck. Objectives: This study aims to provide a systematic review of Deep Learning (DL) approaches to ARRG. Specifically, it aims to answer the following research questions. What data have been used to train and evaluate DL approaches to ARRG? How are DL approaches to ARRG evaluated? How is DL used to generate the reports from radiology images? Materials and methods: We followed the PRISMA guidelines. We retrieved 1443 records from PubMed and Web of Science on November 3, 2021. Relevant studies were categorized and compared from multiple perspectives. The corresponding findings were reported narratively. Results: A total of 41 studies were included. We identified 14 radiology datasets. In terms of evaluation, we identified four commonly used natural language generation metrics, six clinical efficacy metrics, and other qualitative methods. We compared DL approaches with respect to the underlying neural network architecture, the method of text generation, problem representation, training strategy, interpretability, and intermediate processing. Discussion and conclusion: Data imbalance (normal versus abnormal cases) and the inner complexity of reports pose major difficulties in ARRG. More appropriate evaluation metrics are required as well as datasets on a much larger scale. Leveraging structured representation of radiology reports and pre-trained language models warrant further research. \u00a9 2023", "author keywords": "Deep learning; Image processing; Natural language generation; Natural language processing; Neural network", "index keywords": "controlled study; deep learning; human; image processing; Medline; natural language processing; Preferred Reporting Items for Systematic Reviews and Meta-Analyses; qualitative analysis; radiology; review; systematic review; Web of Science", "document type": "Review", "publication stage": "Final", "open access": "All Open Access; Gold Open Access; Green Open Access", "source": "Scopus", "eid": "2-s2.0-85160016936"}, {"authors": "Dreyer K.J.; Kalra M.K.; Maher M.M.; Hurier A.M.; Asfaw B.A.; Schultz T.; Halpern E.F.; Thrall J.H.", "author full names": "Dreyer, Keith J. (7006172475); Kalra, Mannudeep K. (7007035549); Maher, Michael M. (35468364500); Hurier, Autumn M. (16637133300); Asfaw, Benjamin A. (7004420437); Schultz, Thomas (15134494100); Halpern, Elkan F. (24440392500); Thrall, James H. (7006223004)", "author(s) id": "7006172475; 7007035549; 35468364500; 16637133300; 7004420437; 15134494100; 24440392500; 7006223004", "title": "Application of recently developed computer algorithm for automatic classification of unstructured radiology reports: Validation study", "year": 2005, "source title": "Radiology", "volume": 234, "issue": 2.0, "art. no.": NaN, "page start": "323", "page end": "329", "page count": 6.0, "cited by": 104, "doi": "10.1148/radiol.2341040049", "link": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-12344287992&doi=10.1148%2fradiol.2341040049&partnerID=40&md5=70c315d7b54df9802948760083b24f93", "abstract": "PURPOSE: To validate the accuracy of Lexicon Mediated Entropy Reduction (LEXIMER), a new information theory-based computer algorithm developed by the authors for independent analysis and classification of unstructured radiology reports based on the presence of clinically important findings (FT, where T represents \"true\") and recommendations for subsequent action (RT). MATERIALS AND METHODS: The study was approved by the Human Research Committee of the institutional review board. Consecutive de-identified radiology reports (n = 1059) comprising results of barium studies (n = 99), computed tomography (n = 107), mammography (n = 90), magnetic resonance imaging (n = 108), nuclear medicine (n = 99), positron emission tomography (n = 106), radiography (n = 212), ultrasonography (n = 131), and vascular procedures (n = 107) were independently analyzed by two radiologists and then with LEXIMER to categorize the reports into FT and F T-0 (containing or not containing clinically important findings) categories and RT and RT-0 (containing or not containing recommendations for subsequent action) categories. Accuracy, sensitivity, specificity, and positive and negative predictive values of LEXIMER for placing reports into FT and FT0 and RT and R T0 categories were assessed by using appropriate statistical tests. RESULTS: There was strong interobserver concordance between the two radiologists for placing radiology reports into FT and RT categories (K = 0.9, P < .01). For the LEXIMER program, accuracy, sensitivity, specificity, and positive and negative predictive values, respectively, were 97.5% (95% confidence interval [Cl]: 96.6%, 98.5%), 98.9% (95% Cl: 97.9%, 99.6%), 94.9% (95% Cl: 93.1%, 96.0%), 97.5% (95% Cl: 96.6%, 98.0%), and 97.7% (95% Cl: 95.8%, 98.8%) for placing radiology reports into FT and FT0 categories and 99.6% (95% Cl: 99.2%, 99.9%), 98.2% (95% Cl: 95.0%, 99.6%), 99.9% (95% Cl: 99.4%, 99.99%), 99.4% (95% Cl: 96.3%, 99.9%), and 99.7% (95% Cl: 98.9%, 99.9%) for placing reports into RT and R TO categories. CONCLUSION: LEXIMER is an accurate automated engine for evaluating the percentage positivity of clinically important findings and rates of recommendation for subsequent action in unstructured radiology reports. \u00a9 RSNA, 2004.", "author keywords": NaN, "index keywords": "Algorithms; Decision Making, Computer-Assisted; False Negative Reactions; False Positive Reactions; Humans; Magnetic Resonance Imaging; Mammography; Observer Variation; Radiology Information Systems; Radionuclide Imaging; Sensitivity and Specificity; Tomography, Emission-Computed, Single-Photon; Tomography, X-Ray Computed; Ultrasonography; barium; accuracy; algorithm; analysis; classification; computer assisted tomography; computer program; data base; echography; human; information system; lexicon mediated entropy reduction; mammography; nuclear magnetic resonance imaging; nuclear medicine; positron emission tomography; prediction and forecasting; priority journal; radiography; radiologist; radiology; review; sensitivity and specificity; statistical analysis", "document type": "Review", "publication stage": "Final", "open access": NaN, "source": "Scopus", "eid": "2-s2.0-12344287992"}, {"authors": "Murugan V.A.; Chatfield M.B.; Rehani M.; Kalra M.K.", "author full names": "Murugan, Venkatesh A. (56841382500); Chatfield, Mythreyi B. (6602216736); Rehani, Madan (7003893393); Kalra, Mannudeep K. (7007035549)", "author(s) id": "56841382500; 6602216736; 7003893393; 7007035549", "title": "ACR DIR: A user's guide for cardiothoracic radiologists: Part 2: How to interpret your DIR report", "year": 2015, "source title": "Journal of Thoracic Imaging", "volume": 30, "issue": 6.0, "art. no.": NaN, "page start": "W69", "page end": "W72", "page count": 3.0, "cited by": 5, "doi": "10.1097/rti.0000000000000152", "link": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944876731&doi=10.1097%2frti.0000000000000152&partnerID=40&md5=47a195788cbaaeca917b972b8f93a219", "abstract": "In the initial installment of this 3-part article, we reviewed the role and logistics of the American College of Radiology (ACR) Dose Index Registry (DIR). In this second installment, we review the essential components of ACR DIR and describe how users can interpret their biannual dose reports by benchmarking them against regional or national levels. Understanding these reports can help participating institutions to identify specific protocols or practices that may benefit from changes in order to minimize patient dose while maintaining diagnostic quality examinations. Copyright \u00a9 2015 Wolters Kluwer Health, Inc. All rights reserved.", "author keywords": "Dose Index Registry; Interpreting report; Scenarios", "index keywords": "Automatic Data Processing; Heart; Humans; Radiation Dosage; Radiography, Thoracic; Registries; Societies, Medical; Tomography, X-Ray Computed; United States; cardiothoracic radiologist; comparative study; computer assisted tomography; dose index registry; human; quality control; radiation dose; radiation monitoring; radiologist; register; Review; heart; information processing; medical society; procedures; radiation dose; radiography; register; statistics and numerical data; thorax radiography; United States", "document type": "Review", "publication stage": "Final", "open access": NaN, "source": "Scopus", "eid": "2-s2.0-84944876731"}, {"authors": "Li A.Y.; Elliot N.", "author full names": "Li, Andrew Yu (57205704671); Elliot, Nikki (57190758936)", "author(s) id": "57205704671; 57190758936", "title": "Natural language processing to identify ureteric stones in radiology reports", "year": 2019, "source title": "Journal of Medical Imaging and Radiation Oncology", "volume": 63, "issue": 3.0, "art. no.": NaN, "page start": "307", "page end": "310", "page count": 3.0, "cited by": 15, "doi": "10.1111/1754-9485.12861", "link": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061213866&doi=10.1111%2f1754-9485.12861&partnerID=40&md5=7f8752da717bb5d951fbfd26061c895a", "abstract": "Introduction: Natural language processing (NLP) is an emerging tool which has the ability to automate data extraction from large volumes of unstructured text. One of the main described uses of NLP in radiology is cohort building for epidemiological studies. This study aims to assess the accuracy of NLP in identifying a group of patients positive for ureteric stones on Computed Tomography \u2013 Kidneys, Ureter, Bladder (CT KUB) reports. Methods: Retrospective review of all CT KUB reports in a single calendar year. A locally available NLP tool was used to automatically classify the reports based on positivity for ureteric stones. This was validated by manual review and refined to maximize the accuracy of stone detection. Results: A total of 1874 CT KUB reports were identified. Manual classification of ureteric stone positivity was 36% compared with 27% using NLP. The accuracy of NLP was 85% with a sensitivity of 66% and specificity of 95%. Incorrect classification was due to misspellings, variable syntax, terminology, pluralization and the inability to exclude clinical request details from the search algorithm. Conclusions: Our NLP tool demonstrated high specificity but low sensitivity at identifying CT KUB reports that are positive for ureteric stones. This was attributable to the lack of feature extraction tools tailored for analysing radiology text, incompleteness of the medical lexicon database and heterogeneity of unstructured reports. Improvements in these areas will help improve data extraction accuracy. \u00a9 2019 The Royal Australian and New Zealand College of Radiologists", "author keywords": "information science; natural language processing; renal colic; ureteral calculi; urinary tract imaging", "index keywords": "Big Data; Cross-Sectional Studies; Humans; Natural Language Processing; Radiography, Abdominal; Retrospective Studies; Sensitivity and Specificity; Tomography, X-Ray Computed; Ureteral Calculi; algorithm; automation; computer assisted tomography; cross-sectional study; diagnostic accuracy; disease classification; human; kidney colic; major clinical study; natural language processing; practice guideline; priority journal; retrospective study; Review; sensitivity and specificity; total quality management; ureter stone; validation process; abdominal radiography; diagnostic imaging; ureter stone; x-ray computed tomography", "document type": "Review", "publication stage": "Final", "open access": NaN, "source": "Scopus", "eid": "2-s2.0-85061213866"}, {"authors": "Holder J.; Tocino I.; Facchini D.; Nardecchia N.; Staib L.; Crawley D.; Pahade J.K.", "author full names": "Holder, Justin (57191053644); Tocino, Irena (7003747033); Facchini, David (57218624106); Nardecchia, Nicole (57219292882); Staib, Lawrence (7005557159); Crawley, Dan (57221378942); Pahade, Jay K. (24481636200)", "author(s) id": "57191053644; 7003747033; 57218624106; 57219292882; 7005557159; 57221378942; 24481636200", "title": "Current state of radiology report release in electronic patient portals", "year": 2021, "source title": "Clinical Imaging", "volume": 74, "issue": NaN, "art. no.": NaN, "page start": "22", "page end": "26", "page count": 4.0, "cited by": 8, "doi": "10.1016/j.clinimag.2020.12.020", "link": "https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098959242&doi=10.1016%2fj.clinimag.2020.12.020&partnerID=40&md5=4601bb3e087975fdd39c3d7844246084", "abstract": "Objective: The aim of our study is to evaluate the current practice patterns of radiology report release into electronic patient portals. Methods: A survey to assess details of radiology report release was distributed to members of The Association of Administrators in Academic Radiology across the United States. Numerical analysis was used to calculate the frequencies and percentages for the clinical site, frequency and pattern of patient portal use were calculated. Statistical analysis determined the percentages and frequencies for the clinical site, frequency and pattern of patient portal use, as well as statistical differences. Results: A total of 31 (response rate = 28%, 31/108) at least partially completed surveys were received. Most (29/31, 94%) sites reported having a patient portal available with 80% (12/15) reporting < 50% patient utilization. There were no significant (p > 0.05) geographical differences noted in percentage utilization. Seventy-eight percent (21/27) of sites reported some form of automatic radiology report release into their portal. Mean delay was 4 days (range 0\u20137) from report completion to portal release. No correlation (r = 2) was seen between percentage of patient utilization of portals and timing of radiology report release. Conclusion: Most academic centers across the country have patient portals, however, most of these centers report less than 50% utilization of the portals by patients. While variability in radiology report release in patient portals was noted, the majority (78%) of academic medical centers have some form of automatic report release with average delay of 4 days between report completion to portal release. \u00a9 2021 Elsevier Inc.", "author keywords": "Electronic medical record; Patient centered care; Patient portal; Radiology reports; Radiology results", "index keywords": "Electronics; Humans; Patient Portals; Radiography; Radiology; Surveys and Questionnaires; United States; Radiology; Surveys; Academic medical centers; Average delay; Clinical sites; Current practices; Patient portal; Radiology reports; Response rate; Statistical differences; administrative personnel; adult; case report; clinical article; electronic medical record; female; human; male; patient care; radiology; review; United States; university hospital; electronics; medical record; questionnaire; radiography; Radiation", "document type": "Review", "publication stage": "Final", "open access": NaN, "source": "Scopus", "eid": "2-s2.0-85098959242"}]}